{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REID_DEMO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33b274604c7744319539cf83cd67b941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c881a11cebe452db944c8dbb314c69a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49b95b41332d426ea16bd541bc4d91ab",
              "IPY_MODEL_7c9efa618fe44e0b82968f02eba72146",
              "IPY_MODEL_8ff2cfc901b54863afa2ac72cb44d921"
            ]
          }
        },
        "0c881a11cebe452db944c8dbb314c69a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49b95b41332d426ea16bd541bc4d91ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6fd3b4d09d74436c92198dbb07e6e127",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4274cb88413248e3a2817b643c6a357d"
          }
        },
        "7c9efa618fe44e0b82968f02eba72146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efc7c20253984659a825db15fa43944e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d47cd228e4f24403b6790643fd618f56"
          }
        },
        "8ff2cfc901b54863afa2ac72cb44d921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10bf83c32e264ea48e893209232c6cbb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 73.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5ffad72af1248c8ac593c0024b15fa4"
          }
        },
        "6fd3b4d09d74436c92198dbb07e6e127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4274cb88413248e3a2817b643c6a357d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efc7c20253984659a825db15fa43944e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d47cd228e4f24403b6790643fd618f56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10bf83c32e264ea48e893209232c6cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5ffad72af1248c8ac593c0024b15fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHPNQ0C7YNtv",
        "outputId": "9a394592-94e3-4e99-e499-19f59410b6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Neural Network & Deep Learning\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd /content/drive/My Drive/Neural Network & Deep Learning/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import random\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from prefetch_generator import BackgroundGenerator\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "from collections import defaultdict\n",
        "from math import sqrt\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "cudnn.enabled = True\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "JBYyFpqWd1Gq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderX(DataLoader):\n",
        "    def __iter__(self):\n",
        "        return BackgroundGenerator(super().__iter__())"
      ],
      "metadata": {
        "id": "pdXgZkalEtYT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super().__init__()\n",
        "        self.globalavgpooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(c_in, max(1, c_in // 16))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(max(1, c_in // 16), c_in)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.c_in = c_in\n",
        "    \n",
        "    def forward(self, x):\n",
        "        assert self.c_in == x.size(1)\n",
        "        x = self.globalavgpooling(x)\n",
        "        x = x.squeeze()\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SEDense18(nn.Module):\n",
        "    def __init__(self, num_class=751, needs_norm=True, is_reid=False):\n",
        "        super().__init__()\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.basicBlock11 = model.layer1[0]\n",
        "        self.seblock1 = SEBlock(64)\n",
        "\n",
        "        self.basicBlock12 = model.layer1[1]\n",
        "        self.seblock2 = SEBlock(64)\n",
        "\n",
        "        self.basicBlock21 = model.layer2[0]\n",
        "        self.seblock3 = SEBlock(128)\n",
        "        self.ancillaryconv3 = nn.Conv2d(64, 128, 1, 2, 0)\n",
        "        self.optionalNorm2dconv3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.basicBlock22 = model.layer2[1]\n",
        "        self.seblock4 = SEBlock(128)\n",
        "\n",
        "        self.basicBlock31 = model.layer3[0]\n",
        "        self.seblock5 = SEBlock(256)\n",
        "        self.ancillaryconv5 = nn.Conv2d(128, 256, 1, 2, 0)\n",
        "        self.optionalNorm2dconv5 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.basicBlock32 = model.layer3[1]\n",
        "        self.seblock6 = SEBlock(256)\n",
        "\n",
        "        self.basicBlock41 = model.layer4[0]\n",
        "        self.seblock7 = SEBlock(512)\n",
        "        self.ancillaryconv7 = nn.Conv2d(256, 512, 1, 2, 0)\n",
        "        self.optionalNorm2dconv7 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.basicBlock42 = model.layer4[1]\n",
        "        self.seblock8 = SEBlock(512)\n",
        "\n",
        "        self.avgpooling = model.avgpool\n",
        "        # self.fc = nn.Linear(512, num_class)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, num_class),\n",
        "        )\n",
        "        self.needs_norm = needs_norm\n",
        "        self.is_reid = is_reid\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "        branch1 = x\n",
        "        x = self.basicBlock11(x)\n",
        "        scale1 = self.seblock1(x)\n",
        "        x = scale1 * x + branch1\n",
        "\n",
        "        branch2 = x\n",
        "        x = self.basicBlock12(x)\n",
        "        scale2 = self.seblock2(x)\n",
        "        x = scale2 * x + branch2\n",
        "\n",
        "        branch3 = x\n",
        "        x = self.basicBlock21(x)\n",
        "        scale3 = self.seblock3(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale3 * x + self.optionalNorm2dconv3(self.ancillaryconv3(branch3))\n",
        "        else:\n",
        "            x = scale3 * x + self.ancillaryconv3(branch3)\n",
        "\n",
        "        branch4 = x\n",
        "        x = self.basicBlock22(x)\n",
        "        scale4 = self.seblock4(x)\n",
        "        x = scale4 * x + branch4\n",
        "\n",
        "        branch5 = x\n",
        "        x = self.basicBlock31(x)\n",
        "        scale5 = self.seblock5(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale5 * x + self.optionalNorm2dconv5(self.ancillaryconv5(branch5))\n",
        "        else:\n",
        "            x = scale5 * x + self.ancillaryconv5(branch5)\n",
        "\n",
        "        branch6 = x\n",
        "        x = self.basicBlock32(x)\n",
        "        scale6 = self.seblock6(x)\n",
        "        x = scale6 * x + branch6\n",
        "\n",
        "        branch7 = x\n",
        "        x = self.basicBlock41(x)\n",
        "        scale7 = self.seblock7(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale7 * x + self.optionalNorm2dconv7(self.ancillaryconv7(branch7))\n",
        "        else:\n",
        "            x = scale7 * x + self.ancillaryconv7(branch7)\n",
        "\n",
        "        branch8 = x\n",
        "        x = self.basicBlock42(x)\n",
        "        scale8 = self.seblock8(x)\n",
        "        x = scale8 * x + branch8\n",
        "\n",
        "        x = self.avgpooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.is_reid:\n",
        "            return x\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "my_model = SEDense18().cuda()\n",
        "summary(my_model, input_size=(3, 128, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "33b274604c7744319539cf83cd67b941",
            "0c881a11cebe452db944c8dbb314c69a",
            "49b95b41332d426ea16bd541bc4d91ab",
            "7c9efa618fe44e0b82968f02eba72146",
            "8ff2cfc901b54863afa2ac72cb44d921",
            "6fd3b4d09d74436c92198dbb07e6e127",
            "4274cb88413248e3a2817b643c6a357d",
            "efc7c20253984659a825db15fa43944e",
            "d47cd228e4f24403b6790643fd618f56",
            "10bf83c32e264ea48e893209232c6cbb",
            "f5ffad72af1248c8ac593c0024b15fa4"
          ]
        },
        "id": "wcgXv43eYkAu",
        "outputId": "cb6d2f73-34aa-46b6-e695-8e79e385d0e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33b274604c7744319539cf83cd67b941",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 32, 16]               0\n",
            "            Conv2d-5           [-1, 64, 32, 16]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 16]             128\n",
            "              ReLU-7           [-1, 64, 32, 16]               0\n",
            "            Conv2d-8           [-1, 64, 32, 16]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 16]             128\n",
            "             ReLU-10           [-1, 64, 32, 16]               0\n",
            "       BasicBlock-11           [-1, 64, 32, 16]               0\n",
            "AdaptiveAvgPool2d-12             [-1, 64, 1, 1]               0\n",
            "           Linear-13                    [-1, 4]             260\n",
            "             ReLU-14                    [-1, 4]               0\n",
            "           Linear-15                   [-1, 64]             320\n",
            "          Sigmoid-16             [-1, 64, 1, 1]               0\n",
            "          SEBlock-17             [-1, 64, 1, 1]               0\n",
            "           Conv2d-18           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-19           [-1, 64, 32, 16]             128\n",
            "             ReLU-20           [-1, 64, 32, 16]               0\n",
            "           Conv2d-21           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 32, 16]             128\n",
            "             ReLU-23           [-1, 64, 32, 16]               0\n",
            "       BasicBlock-24           [-1, 64, 32, 16]               0\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29             [-1, 64, 1, 1]               0\n",
            "          SEBlock-30             [-1, 64, 1, 1]               0\n",
            "           Conv2d-31           [-1, 128, 16, 8]          73,728\n",
            "      BatchNorm2d-32           [-1, 128, 16, 8]             256\n",
            "             ReLU-33           [-1, 128, 16, 8]               0\n",
            "           Conv2d-34           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-35           [-1, 128, 16, 8]             256\n",
            "           Conv2d-36           [-1, 128, 16, 8]           8,192\n",
            "      BatchNorm2d-37           [-1, 128, 16, 8]             256\n",
            "             ReLU-38           [-1, 128, 16, 8]               0\n",
            "       BasicBlock-39           [-1, 128, 16, 8]               0\n",
            "AdaptiveAvgPool2d-40            [-1, 128, 1, 1]               0\n",
            "           Linear-41                    [-1, 8]           1,032\n",
            "             ReLU-42                    [-1, 8]               0\n",
            "           Linear-43                  [-1, 128]           1,152\n",
            "          Sigmoid-44            [-1, 128, 1, 1]               0\n",
            "          SEBlock-45            [-1, 128, 1, 1]               0\n",
            "           Conv2d-46           [-1, 128, 16, 8]           8,320\n",
            "      BatchNorm2d-47           [-1, 128, 16, 8]             256\n",
            "           Conv2d-48           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-49           [-1, 128, 16, 8]             256\n",
            "             ReLU-50           [-1, 128, 16, 8]               0\n",
            "           Conv2d-51           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-52           [-1, 128, 16, 8]             256\n",
            "             ReLU-53           [-1, 128, 16, 8]               0\n",
            "       BasicBlock-54           [-1, 128, 16, 8]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59            [-1, 128, 1, 1]               0\n",
            "          SEBlock-60            [-1, 128, 1, 1]               0\n",
            "           Conv2d-61            [-1, 256, 8, 4]         294,912\n",
            "      BatchNorm2d-62            [-1, 256, 8, 4]             512\n",
            "             ReLU-63            [-1, 256, 8, 4]               0\n",
            "           Conv2d-64            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-65            [-1, 256, 8, 4]             512\n",
            "           Conv2d-66            [-1, 256, 8, 4]          32,768\n",
            "      BatchNorm2d-67            [-1, 256, 8, 4]             512\n",
            "             ReLU-68            [-1, 256, 8, 4]               0\n",
            "       BasicBlock-69            [-1, 256, 8, 4]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 256, 1, 1]               0\n",
            "           Linear-71                   [-1, 16]           4,112\n",
            "             ReLU-72                   [-1, 16]               0\n",
            "           Linear-73                  [-1, 256]           4,352\n",
            "          Sigmoid-74            [-1, 256, 1, 1]               0\n",
            "          SEBlock-75            [-1, 256, 1, 1]               0\n",
            "           Conv2d-76            [-1, 256, 8, 4]          33,024\n",
            "      BatchNorm2d-77            [-1, 256, 8, 4]             512\n",
            "           Conv2d-78            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-79            [-1, 256, 8, 4]             512\n",
            "             ReLU-80            [-1, 256, 8, 4]               0\n",
            "           Conv2d-81            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-82            [-1, 256, 8, 4]             512\n",
            "             ReLU-83            [-1, 256, 8, 4]               0\n",
            "       BasicBlock-84            [-1, 256, 8, 4]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89            [-1, 256, 1, 1]               0\n",
            "          SEBlock-90            [-1, 256, 1, 1]               0\n",
            "           Conv2d-91            [-1, 512, 4, 2]       1,179,648\n",
            "      BatchNorm2d-92            [-1, 512, 4, 2]           1,024\n",
            "             ReLU-93            [-1, 512, 4, 2]               0\n",
            "           Conv2d-94            [-1, 512, 4, 2]       2,359,296\n",
            "      BatchNorm2d-95            [-1, 512, 4, 2]           1,024\n",
            "           Conv2d-96            [-1, 512, 4, 2]         131,072\n",
            "      BatchNorm2d-97            [-1, 512, 4, 2]           1,024\n",
            "             ReLU-98            [-1, 512, 4, 2]               0\n",
            "       BasicBlock-99            [-1, 512, 4, 2]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 512, 1, 1]               0\n",
            "          Linear-101                   [-1, 32]          16,416\n",
            "            ReLU-102                   [-1, 32]               0\n",
            "          Linear-103                  [-1, 512]          16,896\n",
            "         Sigmoid-104            [-1, 512, 1, 1]               0\n",
            "         SEBlock-105            [-1, 512, 1, 1]               0\n",
            "          Conv2d-106            [-1, 512, 4, 2]         131,584\n",
            "     BatchNorm2d-107            [-1, 512, 4, 2]           1,024\n",
            "          Conv2d-108            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-109            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-110            [-1, 512, 4, 2]               0\n",
            "          Conv2d-111            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-112            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-113            [-1, 512, 4, 2]               0\n",
            "      BasicBlock-114            [-1, 512, 4, 2]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119            [-1, 512, 1, 1]               0\n",
            "         SEBlock-120            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-121            [-1, 512, 1, 1]               0\n",
            "          Linear-122                  [-1, 256]         131,328\n",
            "     BatchNorm1d-123                  [-1, 256]             512\n",
            "            ReLU-124                  [-1, 256]               0\n",
            "         Dropout-125                  [-1, 256]               0\n",
            "          Linear-126                  [-1, 751]         193,007\n",
            "================================================================\n",
            "Total params: 11,765,159\n",
            "Trainable params: 11,765,159\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 10.77\n",
            "Params size (MB): 44.88\n",
            "Estimated Total Size (MB): 55.74\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_path, pose_path, label_path, transform):\n",
        "        super().__init__()\n",
        "        self.image_path = image_path\n",
        "        self.pose_path = pose_path\n",
        "        self.label_path = label_path\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image1 = self.image_path[idx]\n",
        "        pose1 = self.pose_path[idx]\n",
        "        random_index = random.choice([i for i in range(len(self.image_path)) if i != idx])\n",
        "        image2 = self.image_path[random_index]\n",
        "        pose2 = self.pose_path[random_index]\n",
        "        relative_label = 0 if self.label_path[idx] == self.label_path[random_index] else 1\n",
        "        absolute_label = self.label_path[idx]\n",
        "        image1 = Image.open(image1).convert(\"RGB\")\n",
        "        image2 = Image.open(image2).convert(\"RGB\")\n",
        "        pose1 = transforms.ToTensor()(Image.open(pose1).convert(\"L\"))\n",
        "        pose2 = transforms.ToTensor()(Image.open(pose2).convert(\"L\"))\n",
        "        if self.transform:\n",
        "            image1 = self.transform(image1)\n",
        "            image2 = self.transform(image2)\n",
        "        relative_label = torch.tensor(relative_label).int()\n",
        "        absolute_label = torch.tensor(absolute_label).int()\n",
        "        return image1, image2, pose1, pose2, relative_label, absolute_label\n",
        "\n",
        "\n",
        "def relabel(label_set):\n",
        "    label = 0\n",
        "    latest_label = label_set[0]\n",
        "    new_label_set = list()\n",
        "    for cur_label in label_set:\n",
        "        if cur_label != latest_label:\n",
        "            label += 1\n",
        "            latest_label = cur_label\n",
        "        new_label_set.append(label)\n",
        "    return new_label_set"
      ],
      "metadata": {
        "id": "O8nJr2Q1dwhh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this happens within batch\n",
        "# could we have anchor images? -> maybe we need to switch to triplet loss\n",
        "\n",
        "\n",
        "class ContrastiveLossGoogle(nn.Module):\n",
        "    def __init__(self, batch_size, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
        "        self.register_buffer(\"negatives_mask\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
        "            \n",
        "    def forward(self, emb_i, emb_j):\n",
        "        \"\"\"\n",
        "        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs\n",
        "        z_i, z_j as per SimCLR paper\n",
        "        \"\"\"\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
        "        \n",
        "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
        "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
        "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
        "        \n",
        "        nominator = torch.exp(positives / self.temperature)\n",
        "        denominator = self.negatives_mask * torch.exp(similarity_matrix / self.temperature)\n",
        "    \n",
        "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
        "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "2balSilIatnc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class contrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=20.0):\n",
        "        super(contrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"\"\" NLL loss with label smoothing. \"\"\"\n",
        "\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\" Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor \"\"\"\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "        target = target.long()\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "D4tuCWA77Kbb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I find it hard to do strict contrastive training since for the same person, there may be pose issues."
      ],
      "metadata": {
        "id": "LM58WQta-KaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseLoss(nn.Module):\n",
        "    def __init__(self, margin=None):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, pose1, pose2):\n",
        "        return torch.log(torch.clamp(torch.abs(torch.sum(pose1) - torch.sum(pose2)), 0.) + 1e-6)\n",
        "\n",
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, margin=20.0):\n",
        "        super().__init__()\n",
        "        self.contrastive = contrastiveLoss(margin)\n",
        "        self.pose = PoseLoss()\n",
        "    \n",
        "    def forward(self, feature1, feature2, label, pose1, pose2):\n",
        "        return 0.5 * self.contrastive(feature1, feature2, label) + 0.5 * self.pose(pose1, pose2)"
      ],
      "metadata": {
        "id": "v2m11AsaA0s1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_strategy(num_classes):\n",
        "    # The followings are used for training the embedding network\n",
        "    embedding = SEDense18(num_class=num_classes, is_reid=True).cuda()\n",
        "    # loss_function_embedding = contrastiveLoss(margin=300)\n",
        "    loss_function_embedding = HybridLoss(margin=200)\n",
        "    optimizer_embedding = torch.optim.Adam(embedding.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    lr_scheduler_embedding = torch.optim.lr_scheduler.StepLR(optimizer_embedding, step_size=2000, gamma=0.5)\n",
        "    # The followings are used for training the classification network\n",
        "    classifier = embedding.classifier\n",
        "    loss_function_classifier = LabelSmoothing()\n",
        "    optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    lr_scheduler_classifier = torch.optim.lr_scheduler.StepLR(optimizer_classifier, step_size=2000, gamma=0.5)\n",
        "    return embedding, loss_function_embedding, optimizer_embedding, lr_scheduler_embedding, \\\n",
        "        classifier, loss_function_classifier, optimizer_classifier, lr_scheduler_classifier\n"
      ],
      "metadata": {
        "id": "HvTLDxqL0xqP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import TensorDataset\n",
        "\n",
        "\n",
        "def train(image_path, pose_path, label_path, num_class, epochs=10, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 64)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        transforms.RandomErasing(),                           \n",
        "    ])\n",
        "    losses_embed = list()\n",
        "    losses_class = list()\n",
        "    embed_model, loss_embed, optim_embed, lr_embed, class_model, loss_class, optim_class, lr_class = training_strategy(num_class)\n",
        "    embed_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        reid_dataset = MyDataset(image_path, pose_path, label_path, transform)\n",
        "        dataloader = DataLoaderX(reid_dataset, batch_size, True, num_workers=4, pin_memory=True)\n",
        "        iterator = tqdm(dataloader)\n",
        "        for sample in iterator:\n",
        "            optim_embed.zero_grad()\n",
        "            image1, image2, pose1, pose2, labels, _ = sample\n",
        "            image1 = image1.cuda()\n",
        "            image2 = image2.cuda()\n",
        "            labels = labels.cuda()\n",
        "            feature1 = embed_model(image1)\n",
        "            feature2 = embed_model(image2)\n",
        "            loss = loss_embed(feature1, feature2, labels, pose1, pose2)\n",
        "            losses_embed.append(loss.item() / batch_size)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(embed_model.parameters(), 10.)\n",
        "            optim_embed.step()\n",
        "            lr_embed.step()\n",
        "            status = \"epoch: {}, lr: {:.4f}, loss: {:.5f}\".format(epoch, lr_embed.get_last_lr()[0], loss.item() / batch_size)\n",
        "            iterator.set_description(status)\n",
        "    # Once the training is completed, do the inference to obtain the embeddings\n",
        "    embed_model = embed_model.eval()\n",
        "    feature_set = list()\n",
        "    label_set = list()\n",
        "    with torch.no_grad():\n",
        "        dataloader_inference = DataLoaderX(reid_dataset, batch_size, num_workers=4, pin_memory=True)\n",
        "        print(\"Start Inferencing..................\")\n",
        "        for sample in tqdm(dataloader_inference):\n",
        "            image1, _, _, _, _, labels = sample\n",
        "            image1 = image1.cuda()\n",
        "            feature1 = embed_model(image1)\n",
        "            feature_set.append(feature1.detach().cpu())\n",
        "            label_set.append(labels.detach().cpu())\n",
        "    feature_set = torch.cat(feature_set, dim=0)\n",
        "    label_set = torch.cat(label_set, dim=0)\n",
        "    classDataset = TensorDataset(feature_set, label_set)\n",
        "\n",
        "    class_model.train()\n",
        "    for epoch in range(1):\n",
        "        dataloader = DataLoaderX(classDataset, batch_size, True, num_workers=4, pin_memory=True)\n",
        "        iterator = tqdm(dataloader)\n",
        "        for sample in iterator:\n",
        "            optim_class.zero_grad()\n",
        "            embed, label = sample\n",
        "            embed = embed.cuda()\n",
        "            label = label.cuda()\n",
        "            prediction = class_model(embed)\n",
        "            loss = loss_class(prediction, label)\n",
        "            losses_class.append(loss.item() / batch_size)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(class_model.parameters(), 10.)\n",
        "            optim_class.step()\n",
        "            lr_class.step()\n",
        "            status = \"epoch: {}, lr: {:.4f}, loss: {:.5f}\".format(epoch, lr_embed.get_last_lr()[0], loss.item() / batch_size)\n",
        "            iterator.set_description(status)\n",
        "    class_model = class_model.eval()\n",
        "    return embed_model, class_model, losses_embed, losses_class\n",
        "\n",
        "\n",
        "def plot_losses(losses_embed, losses_class):\n",
        "    plt.figure()\n",
        "    plt.plot(losses_embed, linewidth=2, color=\"r\", label=\"embedding loss\")\n",
        "    plt.plot(losses_class, linewidth=2, color=\"b\", label=\"classifier loss\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"loss value\")\n",
        "    plt.title(\"loss functions\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yWmKaFCGFDjd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = sorted(glob.glob(\"Market1501/bounding_box_train/*.jpg\"))\n",
        "pose_path = sorted(glob.glob(\"Market1501/bounding_box_train_pose/*.png\"))\n",
        "label_path = list(map(lambda x: int(x.split(\"/\")[-1][:4]), image_path))\n",
        "label_path = relabel(label_path)\n",
        "print(max(label_path))\n",
        "assert len(image_path) == len(pose_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Od8mXAKJt-M",
        "outputId": "673b7560-5058-44a6-b436-2ae64f81c388"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model, class_model, losses_embed, losses_cls = train(image_path, pose_path, label_path, max(label_path)+1, 10)\n",
        "plot_losses(losses_embed, losses_cls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "rljuxyPlO3Js",
        "outputId": "d54787df-84f0-4161-ce0d-d170e0b5da23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch: 0, lr: 0.0100, loss: 12.79590: 100%|██████████| 405/405 [12:17<00:00,  1.82s/it]\n",
            "epoch: 1, lr: 0.0100, loss: 0.86376: 100%|██████████| 405/405 [01:19<00:00,  5.12it/s]\n",
            "epoch: 2, lr: 0.0100, loss: 0.72658: 100%|██████████| 405/405 [01:19<00:00,  5.11it/s]\n",
            "epoch: 3, lr: 0.0100, loss: 0.05107: 100%|██████████| 405/405 [01:19<00:00,  5.11it/s]\n",
            "epoch: 4, lr: 0.0050, loss: 0.04159: 100%|██████████| 405/405 [01:19<00:00,  5.06it/s]\n",
            "epoch: 5, lr: 0.0050, loss: 0.02392: 100%|██████████| 405/405 [01:19<00:00,  5.07it/s]\n",
            "epoch: 6, lr: 0.0050, loss: 3.56168: 100%|██████████| 405/405 [01:19<00:00,  5.07it/s]\n",
            "epoch: 7, lr: 0.0050, loss: 0.05902: 100%|██████████| 405/405 [01:19<00:00,  5.07it/s]\n",
            "epoch: 8, lr: 0.0050, loss: 0.06279: 100%|██████████| 405/405 [01:24<00:00,  4.78it/s]\n",
            "epoch: 9, lr: 0.0025, loss: 1.99773: 100%|██████████| 405/405 [01:19<00:00,  5.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Inferencing..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [01:04<00:00,  6.29it/s]\n",
            "epoch: 0, lr: 0.0025, loss: 0.20301: 100%|██████████| 405/405 [00:03<00:00, 120.78it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall test on image labelling"
      ],
      "metadata": {
        "id": "nKXahSHinFPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "overall_model = SEDense18().cuda()\n",
        "overall_model.load_state_dict(embed_model.state_dict(), strict=False)\n",
        "for key in class_model.state_dict():\n",
        "    new_key = \"classifier.\" + key\n",
        "    class_model.state_dict()[new_key] = class_model.state_dict()[key].clone()\n",
        "overall_model.load_state_dict(class_model.state_dict(), strict=False)\n",
        "# traced_models = torch.jit.trace(overall_model, torch.randn((1,3,128,64)).to(\"cuda\"))\n",
        "# torch.jit.save(traced_models, \"new_model_checkpoint_traced.pt\")\n",
        "torch.save(overall_model.state_dict(), \"new_model_checkpoint.pt\")"
      ],
      "metadata": {
        "id": "rwgLBskPnCNU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform):\n",
        "        super().__init__()\n",
        "        self.image_path = image_path\n",
        "        self.label_path = label_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_path[idx]\n",
        "        label = self.label_path[idx]\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label).int()\n",
        "\n",
        "\n",
        "def test(image_path, label_path, batch_size=32):\n",
        "    overall_model = SEDense18(num_class=max(label_path)+1).cuda()\n",
        "    states = torch.load(\"new_model_checkpoint.pt\", map_location=lambda storage, loc: storage)\n",
        "    overall_model.load_state_dict(states)\n",
        "    overall_model.eval()\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 64)),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),                          \n",
        "    ])\n",
        "    test_dataset = TestDataset(image_path, label_path, transform)\n",
        "    dataloader = DataLoaderX(test_dataset, batch_size, num_workers=4, pin_memory=True)\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        iterator = tqdm(dataloader)\n",
        "        for sample in iterator:\n",
        "            image, label = sample\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "            prediction = torch.argmax(overall_model(image), dim=1)\n",
        "            acc += torch.count_nonzero(torch.eq(prediction, label))\n",
        "    acc = acc / len(image_path)\n",
        "    return acc\n",
        "\n",
        "def test_dataset(path=\"Market1501/bounding_box_train\"):\n",
        "    image_path = sorted(glob.glob(path + \"/*.jpg\"))\n",
        "    label_path = list(map(lambda x: int(x.split(\"/\")[-1][:4]), image_path))\n",
        "    label_path = relabel(label_path)\n",
        "    return image_path, label_path\n",
        "\n"
      ],
      "metadata": {
        "id": "hejnFZzYrrLE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_test, label_path_test = test_dataset()\n",
        "acc = test(image_path_test, label_path_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9TCnuOiYtSk",
        "outputId": "a1b091ed-b36a-46c7-d651-7caf4a716958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 405/405 [00:24<00:00, 16.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS5w2U0cmW3Y",
        "outputId": "dfe63366-97b3-4c2f-a237-b656761072f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022649969905614853"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the dissimilarity of the features. I want to know \n",
        "if my margin is convincing."
      ],
      "metadata": {
        "id": "HEZ8XWZLw9Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product(v1, v2):\n",
        "    return sum(map(lambda x: x[0] * x[1], zip(v1, v2)))\n",
        "\n",
        "def cosine_measure(v1, v2):\n",
        "    prod = dot_product(v1, v2)\n",
        "    len1 = math.sqrt(dot_product(v1, v1))\n",
        "    len2 = math.sqrt(dot_product(v2, v2))\n",
        "    return prod / (len1 * len2)\n",
        "\n",
        "\n",
        "def feature_acquisition(embed_model, image_path, label_path, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 64)),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),                                      \n",
        "    ])\n",
        "    dataset = TestDataset(image_path, label_path, transform)\n",
        "    features = list()\n",
        "    labels = list()\n",
        "    torch.cuda.synchronize()\n",
        "    with torch.no_grad():\n",
        "        dataloader = DataLoaderX(dataset, batch_size, False, num_workers=4, pin_memory=True)\n",
        "        iterator = tqdm(dataloader)\n",
        "        for image, label in iterator:\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "            feature = embed_model(image)\n",
        "            features.append(feature.detach().cpu().numpy())\n",
        "            labels.append(label.detach().cpu().numpy())\n",
        "        features = np.concatenate(features, axis=0)\n",
        "        labels = np.concatenate(labels, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def check_dissimilarity(features, labels):\n",
        "    max_dis_same = 0.\n",
        "    min_dis_diff = 1.\n",
        "    dissimilarity_score = defaultdict(list)\n",
        "    for idx, feature_x in enumerate(features):\n",
        "        for idy, feature_y in enumerate(features):\n",
        "            if idy <= idx:\n",
        "                continue\n",
        "            cosine_sim = cosine_measure(feature_x, feature_y)\n",
        "            if labels[idx] == labels[idy]:\n",
        "                dissimilarity_score[0].append(1 - cosine_sim)\n",
        "                max_dis_same = max(max_dis_same, cosine_sim)\n",
        "            else:\n",
        "                dissimilarity_score[1].append(1 - cosine_sim)\n",
        "                min_dis_diff = min(min_dis_diff, cosine_sim)\n",
        "    return dissimilarity_score, max_dis_same, min_dis_diff\n"
      ],
      "metadata": {
        "id": "900EOvjjw8rj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path, label_path = test_dataset(\"Market1501/bounding_box_test\")"
      ],
      "metadata": {
        "id": "jPCDrH3n43Pz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = feature_acquisition(embed_model, image_path, label_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u36rhRaoB26_",
        "outputId": "35ca1726-4b1f-459a-c678-2b84c7009186"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 410/410 [02:27<00:00,  2.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DGpgIBuwNv6j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0, len(image_path))\n",
        "idy = random.randint(0, len(image_path))\n",
        "dis_sim = 1 - cosine_measure(features[idx], features[idy])\n",
        "# dis_sim = np.sqrt(np.sum((features[idx] - features[idy]) ** 2))\n",
        "print(image_path[idx], image_path[idy], dis_sim, labels[idx] == labels[idy])\n",
        "\n",
        "dis_sim = 1 - cosine_measure(features[1], features[2])\n",
        "# dis_sim = np.sqrt(np.sum((features[0] - features[1]) ** 2))\n",
        "print(image_path[0], image_path[1], dis_sim, labels[0] == labels[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6KNQIuFaUWX",
        "outputId": "4665f919-66dc-4268-cd01-6ac96f59e1ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Market1501/bounding_box_test/0467_c3s1_129883_03.jpg Market1501/bounding_box_test/0561_c4s3_020704_02.jpg 0.6569712384917745 False\n",
            "Market1501/bounding_box_test/0001_c1s1_001051_03.jpg Market1501/bounding_box_test/0001_c1s1_002301_02.jpg 0.5472896785567906 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me calculate the distance between different feature vectors\n",
        "dis1 = np.sqrt(np.sum((features[idx] - features[idy]) ** 2))\n",
        "dis2 = np.sqrt(np.sum((features[0] - features[1]) ** 2))\n",
        "print(\"Different labels\", dis1)\n",
        "print(\"Same labels\", dis2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdkJWqfmg5eN",
        "outputId": "94cd5ff1-7a6d-4c95-9963-823bd1e039c9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different labels 320.73816\n",
            "Same labels 266.7925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take humongous time to run\n",
        "scores, max_dis_same, min_dis_diff = check_dissimilarity(features, labels)"
      ],
      "metadata": {
        "id": "qTefWuCFNrhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max_dis_same is\", max_dis_same)\n",
        "print(\"min_dis_diff is\", min_dis_diff)"
      ],
      "metadata": {
        "id": "9UGjGoQh5VuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the loss function should consider the pose\n",
        "# Intuitively, we can set the loss function = 0.5 * contrastive loss + 0.5 * pose difference\n",
        "# check the pose difference in manhattan distance\n",
        "\n",
        "\n",
        "pose_image_path = sorted(glob.glob(\"Market1501/bounding_box_test_pose/*.jpg\"))\n",
        "pose_image_path = list(filter(lambda x: x.split(\"/\")[-1][0] != \"-\" and x.split(\"/\")[-1][:4] != \"0000\", pose_image_path))\n",
        "pose1 = transforms.ToTensor()(Image.open(pose_image_path[0]).convert(\"L\")).squeeze().numpy()\n",
        "pose2 = transforms.ToTensor()(Image.open(pose_image_path[1]).convert(\"L\")).squeeze().numpy()\n",
        "pose3 = transforms.ToTensor()(Image.open(pose_image_path[2]).convert(\"L\")).squeeze().numpy()\n",
        "print(\"Difference between {} and {} image pose is {}\".format(pose_image_path[0].split(\"/\")[-1], pose_image_path[1].split(\"/\")[-1], abs(pose1.sum() - pose2.sum())))\n",
        "print(\"Difference between {} and {} image pose is {}\".format(pose_image_path[1].split(\"/\")[-1], pose_image_path[2].split(\"/\")[-1], abs(pose2.sum() - pose3.sum())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbVcYksA-YFp",
        "outputId": "091fd6a3-20a2-49fa-9e00-c3c545760a23"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between 0001_c1s1_001051_03_rendered.jpg and 0001_c1s1_002301_02_rendered.jpg image pose is 41.8156852722168\n",
            "Difference between 0001_c1s1_002301_02_rendered.jpg and 0001_c1s1_002401_02_rendered.jpg image pose is 5.450984954833984\n"
          ]
        }
      ]
    }
  ]
}