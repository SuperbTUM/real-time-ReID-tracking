{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REID_DEMO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a044fb9e17e465da91382d2202900e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_91557ddf68dc477eacfccb5701dd4514",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2a083b74bcc94114ab481c13bfd9f992",
              "IPY_MODEL_f8cf187332004eaab2b7871a09a2b1a1",
              "IPY_MODEL_c2d47111e6764641b528f74514f78837"
            ]
          }
        },
        "91557ddf68dc477eacfccb5701dd4514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a083b74bcc94114ab481c13bfd9f992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_446e546a4d8b435d834e8d32c7b68286",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcdcc2d9b174491a899dcdaa5600a3a7"
          }
        },
        "f8cf187332004eaab2b7871a09a2b1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3cd5aa265364611a9a0269eb5a9d3d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f79c7d75290e4dcbad9c0f1a7f20fa61"
          }
        },
        "c2d47111e6764641b528f74514f78837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_800b6fc3516442b8b763d4b5a216e454",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 129MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9abdd934365c40e5a73a59ca275a2c69"
          }
        },
        "446e546a4d8b435d834e8d32c7b68286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcdcc2d9b174491a899dcdaa5600a3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3cd5aa265364611a9a0269eb5a9d3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f79c7d75290e4dcbad9c0f1a7f20fa61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "800b6fc3516442b8b763d4b5a216e454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9abdd934365c40e5a73a59ca275a2c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHPNQ0C7YNtv",
        "outputId": "8ec15346-2908-4a69-c262-d9099d9322dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Neural Network & Deep Learning\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd /content/drive/My Drive/Neural Network & Deep Learning/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import random\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from prefetch_generator import BackgroundGenerator\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "from collections import defaultdict\n",
        "from math import sqrt\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "cudnn.enabled = True\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "JBYyFpqWd1Gq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderX(DataLoader):\n",
        "    def __iter__(self):\n",
        "        return BackgroundGenerator(super().__iter__())"
      ],
      "metadata": {
        "id": "pdXgZkalEtYT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super().__init__()\n",
        "        self.globalavgpooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(c_in, max(1, c_in // 16))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(max(1, c_in // 16), c_in)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.c_in = c_in\n",
        "    \n",
        "    def forward(self, x):\n",
        "        assert self.c_in == x.size(1)\n",
        "        x = self.globalavgpooling(x)\n",
        "        x = x.squeeze()\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SEDense18(nn.Module):\n",
        "    def __init__(self, num_class=751, needs_norm=True, is_reid=False):\n",
        "        super().__init__()\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.basicBlock11 = model.layer1[0]\n",
        "        self.seblock1 = SEBlock(64)\n",
        "\n",
        "        self.basicBlock12 = model.layer1[1]\n",
        "        self.seblock2 = SEBlock(64)\n",
        "\n",
        "        self.basicBlock21 = model.layer2[0]\n",
        "        self.seblock3 = SEBlock(128)\n",
        "        self.ancillaryconv3 = nn.Conv2d(64, 128, 1, 2, 0)\n",
        "        self.optionalNorm2dconv3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.basicBlock22 = model.layer2[1]\n",
        "        self.seblock4 = SEBlock(128)\n",
        "\n",
        "        self.basicBlock31 = model.layer3[0]\n",
        "        self.seblock5 = SEBlock(256)\n",
        "        self.ancillaryconv5 = nn.Conv2d(128, 256, 1, 2, 0)\n",
        "        self.optionalNorm2dconv5 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.basicBlock32 = model.layer3[1]\n",
        "        self.seblock6 = SEBlock(256)\n",
        "\n",
        "        self.basicBlock41 = model.layer4[0]\n",
        "        self.seblock7 = SEBlock(512)\n",
        "        self.ancillaryconv7 = nn.Conv2d(256, 512, 1, 2, 0)\n",
        "        self.optionalNorm2dconv7 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.basicBlock42 = model.layer4[1]\n",
        "        self.seblock8 = SEBlock(512)\n",
        "\n",
        "        self.avgpooling = model.avgpool\n",
        "        # self.fc = nn.Linear(512, num_class)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, num_class),\n",
        "        )\n",
        "        self.needs_norm = needs_norm\n",
        "        self.is_reid = is_reid\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "        branch1 = x\n",
        "        x = self.basicBlock11(x)\n",
        "        scale1 = self.seblock1(x)\n",
        "        x = scale1 * x + branch1\n",
        "\n",
        "        branch2 = x\n",
        "        x = self.basicBlock12(x)\n",
        "        scale2 = self.seblock2(x)\n",
        "        x = scale2 * x + branch2\n",
        "\n",
        "        branch3 = x\n",
        "        x = self.basicBlock21(x)\n",
        "        scale3 = self.seblock3(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale3 * x + self.optionalNorm2dconv3(self.ancillaryconv3(branch3))\n",
        "        else:\n",
        "            x = scale3 * x + self.ancillaryconv3(branch3)\n",
        "\n",
        "        branch4 = x\n",
        "        x = self.basicBlock22(x)\n",
        "        scale4 = self.seblock4(x)\n",
        "        x = scale4 * x + branch4\n",
        "\n",
        "        branch5 = x\n",
        "        x = self.basicBlock31(x)\n",
        "        scale5 = self.seblock5(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale5 * x + self.optionalNorm2dconv5(self.ancillaryconv5(branch5))\n",
        "        else:\n",
        "            x = scale5 * x + self.ancillaryconv5(branch5)\n",
        "\n",
        "        branch6 = x\n",
        "        x = self.basicBlock32(x)\n",
        "        scale6 = self.seblock6(x)\n",
        "        x = scale6 * x + branch6\n",
        "\n",
        "        branch7 = x\n",
        "        x = self.basicBlock41(x)\n",
        "        scale7 = self.seblock7(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale7 * x + self.optionalNorm2dconv7(self.ancillaryconv7(branch7))\n",
        "        else:\n",
        "            x = scale7 * x + self.ancillaryconv7(branch7)\n",
        "\n",
        "        branch8 = x\n",
        "        x = self.basicBlock42(x)\n",
        "        scale8 = self.seblock8(x)\n",
        "        x = scale8 * x + branch8\n",
        "\n",
        "        x = self.avgpooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.is_reid:\n",
        "            return x\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "my_model = SEDense18().cuda()\n",
        "summary(my_model, input_size=(3, 128, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3a044fb9e17e465da91382d2202900e4",
            "91557ddf68dc477eacfccb5701dd4514",
            "2a083b74bcc94114ab481c13bfd9f992",
            "f8cf187332004eaab2b7871a09a2b1a1",
            "c2d47111e6764641b528f74514f78837",
            "446e546a4d8b435d834e8d32c7b68286",
            "bcdcc2d9b174491a899dcdaa5600a3a7",
            "a3cd5aa265364611a9a0269eb5a9d3d4",
            "f79c7d75290e4dcbad9c0f1a7f20fa61",
            "800b6fc3516442b8b763d4b5a216e454",
            "9abdd934365c40e5a73a59ca275a2c69"
          ]
        },
        "id": "wcgXv43eYkAu",
        "outputId": "5a8f6df8-7a22-45d8-ab1a-2cb690310a00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a044fb9e17e465da91382d2202900e4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 32, 16]               0\n",
            "            Conv2d-5           [-1, 64, 32, 16]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 16]             128\n",
            "              ReLU-7           [-1, 64, 32, 16]               0\n",
            "            Conv2d-8           [-1, 64, 32, 16]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 16]             128\n",
            "             ReLU-10           [-1, 64, 32, 16]               0\n",
            "       BasicBlock-11           [-1, 64, 32, 16]               0\n",
            "AdaptiveAvgPool2d-12             [-1, 64, 1, 1]               0\n",
            "           Linear-13                    [-1, 4]             260\n",
            "             ReLU-14                    [-1, 4]               0\n",
            "           Linear-15                   [-1, 64]             320\n",
            "          Sigmoid-16             [-1, 64, 1, 1]               0\n",
            "          SEBlock-17             [-1, 64, 1, 1]               0\n",
            "           Conv2d-18           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-19           [-1, 64, 32, 16]             128\n",
            "             ReLU-20           [-1, 64, 32, 16]               0\n",
            "           Conv2d-21           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 32, 16]             128\n",
            "             ReLU-23           [-1, 64, 32, 16]               0\n",
            "       BasicBlock-24           [-1, 64, 32, 16]               0\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29             [-1, 64, 1, 1]               0\n",
            "          SEBlock-30             [-1, 64, 1, 1]               0\n",
            "           Conv2d-31           [-1, 128, 16, 8]          73,728\n",
            "      BatchNorm2d-32           [-1, 128, 16, 8]             256\n",
            "             ReLU-33           [-1, 128, 16, 8]               0\n",
            "           Conv2d-34           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-35           [-1, 128, 16, 8]             256\n",
            "           Conv2d-36           [-1, 128, 16, 8]           8,192\n",
            "      BatchNorm2d-37           [-1, 128, 16, 8]             256\n",
            "             ReLU-38           [-1, 128, 16, 8]               0\n",
            "       BasicBlock-39           [-1, 128, 16, 8]               0\n",
            "AdaptiveAvgPool2d-40            [-1, 128, 1, 1]               0\n",
            "           Linear-41                    [-1, 8]           1,032\n",
            "             ReLU-42                    [-1, 8]               0\n",
            "           Linear-43                  [-1, 128]           1,152\n",
            "          Sigmoid-44            [-1, 128, 1, 1]               0\n",
            "          SEBlock-45            [-1, 128, 1, 1]               0\n",
            "           Conv2d-46           [-1, 128, 16, 8]           8,320\n",
            "      BatchNorm2d-47           [-1, 128, 16, 8]             256\n",
            "           Conv2d-48           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-49           [-1, 128, 16, 8]             256\n",
            "             ReLU-50           [-1, 128, 16, 8]               0\n",
            "           Conv2d-51           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-52           [-1, 128, 16, 8]             256\n",
            "             ReLU-53           [-1, 128, 16, 8]               0\n",
            "       BasicBlock-54           [-1, 128, 16, 8]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59            [-1, 128, 1, 1]               0\n",
            "          SEBlock-60            [-1, 128, 1, 1]               0\n",
            "           Conv2d-61            [-1, 256, 8, 4]         294,912\n",
            "      BatchNorm2d-62            [-1, 256, 8, 4]             512\n",
            "             ReLU-63            [-1, 256, 8, 4]               0\n",
            "           Conv2d-64            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-65            [-1, 256, 8, 4]             512\n",
            "           Conv2d-66            [-1, 256, 8, 4]          32,768\n",
            "      BatchNorm2d-67            [-1, 256, 8, 4]             512\n",
            "             ReLU-68            [-1, 256, 8, 4]               0\n",
            "       BasicBlock-69            [-1, 256, 8, 4]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 256, 1, 1]               0\n",
            "           Linear-71                   [-1, 16]           4,112\n",
            "             ReLU-72                   [-1, 16]               0\n",
            "           Linear-73                  [-1, 256]           4,352\n",
            "          Sigmoid-74            [-1, 256, 1, 1]               0\n",
            "          SEBlock-75            [-1, 256, 1, 1]               0\n",
            "           Conv2d-76            [-1, 256, 8, 4]          33,024\n",
            "      BatchNorm2d-77            [-1, 256, 8, 4]             512\n",
            "           Conv2d-78            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-79            [-1, 256, 8, 4]             512\n",
            "             ReLU-80            [-1, 256, 8, 4]               0\n",
            "           Conv2d-81            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-82            [-1, 256, 8, 4]             512\n",
            "             ReLU-83            [-1, 256, 8, 4]               0\n",
            "       BasicBlock-84            [-1, 256, 8, 4]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89            [-1, 256, 1, 1]               0\n",
            "          SEBlock-90            [-1, 256, 1, 1]               0\n",
            "           Conv2d-91            [-1, 512, 4, 2]       1,179,648\n",
            "      BatchNorm2d-92            [-1, 512, 4, 2]           1,024\n",
            "             ReLU-93            [-1, 512, 4, 2]               0\n",
            "           Conv2d-94            [-1, 512, 4, 2]       2,359,296\n",
            "      BatchNorm2d-95            [-1, 512, 4, 2]           1,024\n",
            "           Conv2d-96            [-1, 512, 4, 2]         131,072\n",
            "      BatchNorm2d-97            [-1, 512, 4, 2]           1,024\n",
            "             ReLU-98            [-1, 512, 4, 2]               0\n",
            "       BasicBlock-99            [-1, 512, 4, 2]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 512, 1, 1]               0\n",
            "          Linear-101                   [-1, 32]          16,416\n",
            "            ReLU-102                   [-1, 32]               0\n",
            "          Linear-103                  [-1, 512]          16,896\n",
            "         Sigmoid-104            [-1, 512, 1, 1]               0\n",
            "         SEBlock-105            [-1, 512, 1, 1]               0\n",
            "          Conv2d-106            [-1, 512, 4, 2]         131,584\n",
            "     BatchNorm2d-107            [-1, 512, 4, 2]           1,024\n",
            "          Conv2d-108            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-109            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-110            [-1, 512, 4, 2]               0\n",
            "          Conv2d-111            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-112            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-113            [-1, 512, 4, 2]               0\n",
            "      BasicBlock-114            [-1, 512, 4, 2]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119            [-1, 512, 1, 1]               0\n",
            "         SEBlock-120            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-121            [-1, 512, 1, 1]               0\n",
            "          Linear-122                  [-1, 256]         131,328\n",
            "     BatchNorm1d-123                  [-1, 256]             512\n",
            "            ReLU-124                  [-1, 256]               0\n",
            "         Dropout-125                  [-1, 256]               0\n",
            "          Linear-126                  [-1, 751]         193,007\n",
            "================================================================\n",
            "Total params: 11,765,159\n",
            "Trainable params: 11,765,159\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 10.77\n",
            "Params size (MB): 44.88\n",
            "Estimated Total Size (MB): 55.74\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_path, pose_path, label_path, transform):\n",
        "        super().__init__()\n",
        "        self.image_path = image_path\n",
        "        self.pose_path = pose_path\n",
        "        self.label_path = label_path\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image1 = self.image_path[idx]\n",
        "        pose1 = self.pose_path[idx]\n",
        "        random_index = random.choice([i for i in range(len(self.image_path)) if i != idx])\n",
        "        image2 = self.image_path[random_index]\n",
        "        pose2 = self.pose_path[random_index]\n",
        "        relative_label = 0 if self.label_path[idx] == self.label_path[random_index] else 1\n",
        "        absolute_label = self.label_path[idx]\n",
        "        image1 = Image.open(image1).convert(\"RGB\")\n",
        "        image2 = Image.open(image2).convert(\"RGB\")\n",
        "        pose1 = transforms.ToTensor()(Image.open(pose1).convert(\"L\"))\n",
        "        pose2 = transforms.ToTensor()(Image.open(pose2).convert(\"L\"))\n",
        "        if self.transform:\n",
        "            image1 = self.transform(image1)\n",
        "            image2 = self.transform(image2)\n",
        "        relative_label = torch.tensor(relative_label).int()\n",
        "        absolute_label = torch.tensor(absolute_label).int()\n",
        "        return image1, image2, pose1, pose2, relative_label, absolute_label\n",
        "\n",
        "\n",
        "class CenTriDataset(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform):\n",
        "        self.image_path = image_path\n",
        "        self.label_path = label_path\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_path[idx]\n",
        "        label = self.label_path[idx]\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label).int()\n",
        "\n",
        "\n",
        "def relabel(label_set):\n",
        "    label = 0\n",
        "    latest_label = label_set[0]\n",
        "    new_label_set = list()\n",
        "    for cur_label in label_set:\n",
        "        if cur_label != latest_label:\n",
        "            label += 1\n",
        "            latest_label = cur_label\n",
        "        new_label_set.append(label)\n",
        "    return new_label_set"
      ],
      "metadata": {
        "id": "O8nJr2Q1dwhh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class contrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=20.0):\n",
        "        super(contrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"\"\" NLL loss with label smoothing. \"\"\"\n",
        "\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\" Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor \"\"\"\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "        target = target.long()\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "D4tuCWA77Kbb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding center loss and triplet to train the network based on strong baseline paper."
      ],
      "metadata": {
        "id": "i-KUr3ZoRG_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"Center loss.\n",
        "    Reference:\n",
        "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        feat_dim (int): feature dimension.\n",
        "    \"\"\"\n",
        " \n",
        "    def __init__(self, num_classes=751, feat_dim=2048, use_gpu=True):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        self.use_gpu = use_gpu\n",
        " \n",
        "        if self.use_gpu:\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
        "        else:\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
        " \n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (num_classes).\n",
        "        \"\"\"\n",
        "        assert x.size(0) == labels.size(0), \"features.size(0) is not equal to labels.size(0)\"\n",
        " \n",
        "        batch_size = x.size(0)\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\n",
        " \n",
        "        classes = torch.arange(self.num_classes).long()\n",
        "        if self.use_gpu: classes = classes.cuda()\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        " \n",
        "        dist = []\n",
        "        for i in range(batch_size):\n",
        "            value = distmat[i][mask[i]]\n",
        "            value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability\n",
        "            dist.append(value)\n",
        "        dist = torch.cat(dist)\n",
        "        loss = dist.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"Triplet loss with hard positive/negative mining.\n",
        "    \n",
        "    Reference:\n",
        "        Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
        "    \n",
        "    Imported from `<https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py>`_.\n",
        "    \n",
        "    Args:\n",
        "        margin (float, optional): margin for triplet. Default is 0.3.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, margin=0.3):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
        " \n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).\n",
        "            targets (torch.LongTensor): ground truth labels with shape (num_classes).\n",
        "        \"\"\"\n",
        "        n = inputs.size(0)\n",
        "        \n",
        "        # Compute pairwise distance, replace by the official when merged\n",
        "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
        "        dist = dist + dist.t()\n",
        "        dist.addmm_(1, -2, inputs, inputs.t())\n",
        "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
        "        \n",
        "        # For each anchor, find the hardest positive and negative\n",
        "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
        "        dist_ap, dist_an = [], []\n",
        "        for i in range(n):\n",
        "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
        "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
        "        dist_ap = torch.cat(dist_ap)\n",
        "        dist_an = torch.cat(dist_an)\n",
        "        \n",
        "        # Compute ranking hinge loss\n",
        "        y = torch.ones_like(dist_an)\n",
        "        return self.ranking_loss(dist_an, dist_ap, y)"
      ],
      "metadata": {
        "id": "QTxROLe7RGhJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I find it hard to do strict contrastive training since for the same person, there may be pose issues."
      ],
      "metadata": {
        "id": "LM58WQta-KaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseLoss(nn.Module):\n",
        "    def __init__(self, margin=None):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, pose1, pose2):\n",
        "        return torch.log(torch.clamp(torch.abs(torch.sum(pose1) - torch.sum(pose2)), 0.) + 1e-6)\n",
        "\n",
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, margin=20.0):\n",
        "        super().__init__()\n",
        "        self.contrastive = contrastiveLoss(margin)\n",
        "        self.pose = PoseLoss()\n",
        "    \n",
        "    def forward(self, feature1, feature2, label, pose1, pose2):\n",
        "        return self.contrastive(feature1, feature2, label) + 0.05 * self.pose(pose1, pose2)\n",
        "\n",
        "\n",
        "class HybridLoss2(nn.Module):\n",
        "    def __init__(self, num_classes, feat_dim=512, margin=200):\n",
        "        super().__init__()\n",
        "        self.center = CenterLoss(num_classes=num_classes, feat_dim=feat_dim)\n",
        "        self.triplet = TripletLoss(margin)\n",
        "    \n",
        "    def forward(self, features, targets):\n",
        "        \"\"\"\n",
        "        features: feature vectors\n",
        "        targets: ground truth labels\n",
        "        \"\"\"\n",
        "        return self.triplet(features, targets) + 0.0005 * self.center(features, targets)"
      ],
      "metadata": {
        "id": "v2m11AsaA0s1"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_strategy(num_classes):\n",
        "    # The followings are used for training the embedding network\n",
        "    embedding = SEDense18(num_class=num_classes, is_reid=True).cuda()\n",
        "    # loss_function_embedding = contrastiveLoss(margin=300)\n",
        "    # loss_function_embedding = HybridLoss(margin=20)\n",
        "    loss_function_embedding = HybridLoss2(num_classes)\n",
        "    optimizer_embedding = torch.optim.Adam(embedding.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "    lr_scheduler_embedding = torch.optim.lr_scheduler.StepLR(optimizer_embedding, step_size=2000, gamma=0.5)\n",
        "    # lr_scheduler_embedding = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_embedding, 10, eta_min=3.5e-5)\n",
        "    # The followings are used for training the classification network\n",
        "    classifier = embedding.classifier\n",
        "    loss_function_classifier = LabelSmoothing()\n",
        "    optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    lr_scheduler_classifier = torch.optim.lr_scheduler.StepLR(optimizer_classifier, step_size=2000, gamma=0.5)\n",
        "    return embedding, loss_function_embedding, optimizer_embedding, lr_scheduler_embedding, \\\n",
        "        classifier, loss_function_classifier, optimizer_classifier, lr_scheduler_classifier\n"
      ],
      "metadata": {
        "id": "HvTLDxqL0xqP"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import TensorDataset\n",
        "\n",
        "\n",
        "def train(image_path, pose_path, label_path, num_class, epochs=10, batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 64)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        transforms.RandomErasing(),                           \n",
        "    ])\n",
        "    reid_dataset = CenTriDataset(image_path, label_path, transform)\n",
        "    losses_embed = list()\n",
        "    losses_class = list()\n",
        "    embed_model, loss_embed, optim_embed, lr_embed, class_model, loss_class, optim_class, lr_class = training_strategy(num_class)\n",
        "    embed_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        # reid_dataset = MyDataset(image_path, pose_path, label_path, transform)\n",
        "        dataloader = DataLoaderX(reid_dataset, batch_size, True, num_workers=4, pin_memory=True)\n",
        "        iterator = tqdm(dataloader)\n",
        "        for sample in iterator:\n",
        "            optim_embed.zero_grad()\n",
        "            # image1, image2, pose1, pose2, labels, _ = sample\n",
        "            image, label = sample\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "            # image1 = image1.cuda()\n",
        "            # image2 = image2.cuda()\n",
        "            # labels = labels.cuda()\n",
        "            # feature1 = embed_model(image1)\n",
        "            # feature2 = embed_model(image2)\n",
        "            feature = embed_model(image)\n",
        "            # loss = loss_embed(feature1, feature2, labels, pose1, pose2)\n",
        "            loss = loss_embed(feature, label)\n",
        "            # losses_embed.append(loss.item() / batch_size)\n",
        "            losses_embed.append(loss.item())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(embed_model.parameters(), 10.)\n",
        "            optim_embed.step()\n",
        "            lr_embed.step()\n",
        "            status = \"epoch: {}, lr: {:.4f}, loss: {:.5f}\".format(epoch, lr_embed.get_last_lr()[0], loss.item() / batch_size)\n",
        "            iterator.set_description(status)\n",
        "    # Once the training is completed, do the inference to obtain the embeddings\n",
        "    embed_model = embed_model.eval()\n",
        "    feature_set = list()\n",
        "    label_set = list()\n",
        "    with torch.no_grad():\n",
        "        dataloader_inference = DataLoaderX(reid_dataset, batch_size, num_workers=4, pin_memory=True)\n",
        "        print(\"Start Inferencing..................\")\n",
        "        for sample in tqdm(dataloader_inference):\n",
        "            # image1, _, _, _, _, labels = sample\n",
        "            image1, labels = sample\n",
        "            image1 = image1.cuda()\n",
        "            feature1 = embed_model(image1)\n",
        "            feature_set.append(feature1.detach().cpu())\n",
        "            label_set.append(labels.detach().cpu())\n",
        "    feature_set = torch.cat(feature_set, dim=0)\n",
        "    label_set = torch.cat(label_set, dim=0)\n",
        "    classDataset = TensorDataset(feature_set, label_set)\n",
        "\n",
        "    class_model.train()\n",
        "    for epoch in range(1):\n",
        "        dataloader = DataLoaderX(classDataset, batch_size, True, num_workers=4, pin_memory=True)\n",
        "        iterator = tqdm(dataloader)\n",
        "        for sample in iterator:\n",
        "            optim_class.zero_grad()\n",
        "            embed, label = sample\n",
        "            embed = embed.cuda()\n",
        "            label = label.cuda()\n",
        "            prediction = class_model(embed)\n",
        "            loss = loss_class(prediction, label)\n",
        "            losses_class.append(loss.item())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(class_model.parameters(), 10.)\n",
        "            optim_class.step()\n",
        "            lr_class.step()\n",
        "            status = \"epoch: {}, lr: {:.4f}, loss: {:.5f}\".format(epoch, lr_embed.get_last_lr()[0], loss.item() / batch_size)\n",
        "            iterator.set_description(status)\n",
        "    class_model = class_model.eval()\n",
        "    return embed_model, class_model, losses_embed, losses_class\n",
        "\n",
        "\n",
        "def plot_losses(losses_embed, losses_class):\n",
        "    plt.figure()\n",
        "    plt.plot(losses_embed, linewidth=2, color=\"r\", label=\"embedding loss\")\n",
        "    plt.plot(losses_class, linewidth=2, color=\"b\", label=\"classifier loss\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"loss value\")\n",
        "    plt.title(\"loss functions\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yWmKaFCGFDjd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = sorted(glob.glob(\"Market1501/bounding_box_train/*.jpg\"))\n",
        "pose_path = sorted(glob.glob(\"Market1501/bounding_box_train_pose/*.png\"))\n",
        "label_path = list(map(lambda x: int(x.split(\"/\")[-1][:4]), image_path))\n",
        "label_path = relabel(label_path)\n",
        "print(max(label_path))\n",
        "assert len(image_path) == len(pose_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Od8mXAKJt-M",
        "outputId": "843f6395-6765-40a1-f574-b05f30a0f90a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model, class_model, losses_embed, losses_cls = train(image_path, pose_path, label_path, max(label_path)+1, 10)\n",
        "plot_losses(losses_embed, losses_cls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "rljuxyPlO3Js",
        "outputId": "749ab21d-9382-4fd5-b648-92f326549e5f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch: 0, lr: 0.0020, loss: 1.19624: 100%|██████████| 203/203 [00:54<00:00,  3.75it/s]\n",
            "epoch: 1, lr: 0.0020, loss: 0.42457: 100%|██████████| 203/203 [00:53<00:00,  3.82it/s]\n",
            "epoch: 2, lr: 0.0020, loss: 0.38571: 100%|██████████| 203/203 [00:53<00:00,  3.82it/s]\n",
            "epoch: 3, lr: 0.0020, loss: 0.38065: 100%|██████████| 203/203 [00:53<00:00,  3.78it/s]\n",
            "epoch: 4, lr: 0.0020, loss: 0.37449: 100%|██████████| 203/203 [00:53<00:00,  3.83it/s]\n",
            "epoch: 5, lr: 0.0020, loss: 0.34558: 100%|██████████| 203/203 [00:52<00:00,  3.85it/s]\n",
            "epoch: 6, lr: 0.0020, loss: 0.33184: 100%|██████████| 203/203 [00:53<00:00,  3.82it/s]\n",
            "epoch: 7, lr: 0.0020, loss: 0.34072: 100%|██████████| 203/203 [00:52<00:00,  3.84it/s]\n",
            "epoch: 8, lr: 0.0020, loss: 0.31681: 100%|██████████| 203/203 [00:53<00:00,  3.81it/s]\n",
            "epoch: 9, lr: 0.0010, loss: 0.31198: 100%|██████████| 203/203 [00:52<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Inferencing..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 203/203 [00:24<00:00,  8.32it/s]\n",
            "epoch: 0, lr: 0.0010, loss: 0.08819: 100%|██████████| 203/203 [00:02<00:00, 79.35it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall test on image labelling"
      ],
      "metadata": {
        "id": "nKXahSHinFPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "overall_model = SEDense18().cuda()\n",
        "overall_model.load_state_dict(embed_model.state_dict(), strict=False)\n",
        "for key in class_model.state_dict():\n",
        "    new_key = \"classifier.\" + key\n",
        "    class_model.state_dict()[new_key] = class_model.state_dict()[key].clone()\n",
        "overall_model.load_state_dict(class_model.state_dict(), strict=False)\n",
        "# traced_models = torch.jit.trace(overall_model, torch.randn((1,3,128,64)).to(\"cuda\"))\n",
        "# torch.jit.save(traced_models, \"new_model_checkpoint_traced.pt\")\n",
        "torch.save(overall_model.state_dict(), \"new_model_checkpoint.pt\")"
      ],
      "metadata": {
        "id": "rwgLBskPnCNU"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform):\n",
        "        super().__init__()\n",
        "        self.image_path = image_path\n",
        "        self.label_path = label_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_path[idx]\n",
        "        label = self.label_path[idx]\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label).int()\n",
        "\n",
        "\n",
        "def test(image_path, label_path, batch_size=32):\n",
        "    overall_model = SEDense18(num_class=max(label_path)+1).cuda()\n",
        "    states = torch.load(\"new_model_checkpoint.pt\", map_location=lambda storage, loc: storage)\n",
        "    overall_model.load_state_dict(states)\n",
        "    overall_model.eval()\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 64)),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),                          \n",
        "    ])\n",
        "    test_dataset = TestDataset(image_path, label_path, transform)\n",
        "    dataloader = DataLoaderX(test_dataset, batch_size, num_workers=4, pin_memory=True)\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        iterator = tqdm(dataloader)\n",
        "        for sample in iterator:\n",
        "            image, label = sample\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "            prediction = torch.argmax(overall_model(image), dim=1)\n",
        "            acc += torch.count_nonzero(torch.eq(prediction, label))\n",
        "    acc = acc / len(image_path)\n",
        "    return acc\n",
        "\n",
        "def test_dataset(path=\"Market1501/bounding_box_train\"):\n",
        "    image_path = sorted(glob.glob(path + \"/*.jpg\"))\n",
        "    label_path = list(map(lambda x: int(x.split(\"/\")[-1][:4]), image_path))\n",
        "    label_path = relabel(label_path)\n",
        "    return image_path, label_path\n",
        "\n"
      ],
      "metadata": {
        "id": "hejnFZzYrrLE"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_test, label_path_test = test_dataset()\n",
        "acc = test(image_path_test, label_path_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9TCnuOiYtSk",
        "outputId": "f18b646f-4ffe-4a8e-fb4c-d91e8c5f959c"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS5w2U0cmW3Y",
        "outputId": "9d90320d-f12c-4fc4-bd32-8e0303fd0128"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14347557723522186"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the dissimilarity of the features. I want to know \n",
        "if my margin is convincing."
      ],
      "metadata": {
        "id": "HEZ8XWZLw9Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product(v1, v2):\n",
        "    return sum(map(lambda x: x[0] * x[1], zip(v1, v2)))\n",
        "\n",
        "def cosine_measure(v1, v2):\n",
        "    prod = dot_product(v1, v2)\n",
        "    len1 = math.sqrt(dot_product(v1, v1))\n",
        "    len2 = math.sqrt(dot_product(v2, v2))\n",
        "    return min(1., prod / (len1 * len2))\n",
        "\n",
        "\n",
        "def feature_acquisition(embed_model, image_path, label_path, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 64)),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),                                      \n",
        "    ])\n",
        "    dataset = TestDataset(image_path, label_path, transform)\n",
        "    features = list()\n",
        "    labels = list()\n",
        "    torch.cuda.synchronize()\n",
        "    with torch.no_grad():\n",
        "        dataloader = DataLoaderX(dataset, batch_size, False, num_workers=4, pin_memory=True)\n",
        "        iterator = tqdm(dataloader)\n",
        "        for image, label in iterator:\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "            feature = embed_model(image)\n",
        "            features.append(feature.detach().cpu().numpy())\n",
        "            labels.append(label.detach().cpu().numpy())\n",
        "        features = np.concatenate(features, axis=0)\n",
        "        labels = np.concatenate(labels, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def check_dissimilarity(features, labels):\n",
        "    max_dis_same = 0.\n",
        "    min_dis_diff = 1.\n",
        "    dissimilarity_score = defaultdict(list)\n",
        "    for idx, feature_x in enumerate(features):\n",
        "        for idy, feature_y in enumerate(features):\n",
        "            if idy <= idx:\n",
        "                continue\n",
        "            cosine_sim = cosine_measure(feature_x, feature_y)\n",
        "            if labels[idx] == labels[idy]:\n",
        "                dissimilarity_score[0].append(1 - cosine_sim)\n",
        "                max_dis_same = max(max_dis_same, cosine_sim)\n",
        "            else:\n",
        "                dissimilarity_score[1].append(1 - cosine_sim)\n",
        "                min_dis_diff = min(min_dis_diff, cosine_sim)\n",
        "    return dissimilarity_score, max_dis_same, min_dis_diff\n"
      ],
      "metadata": {
        "id": "900EOvjjw8rj"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path, label_path = test_dataset(\"Market1501/bounding_box_test\")"
      ],
      "metadata": {
        "id": "jPCDrH3n43Pz"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = feature_acquisition(embed_model, image_path, label_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u36rhRaoB26_",
        "outputId": "00b83d5d-4f06-4884-d119-f424cc01eef6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 410/410 [00:33<00:00, 12.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DGpgIBuwNv6j"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0, len(image_path))\n",
        "idy = random.randint(0, len(image_path))\n",
        "# features_stacked = np.stack((features[idx], features[idy]))\n",
        "dis_sim = 1 - cosine_measure(features[idx], features[idy])\n",
        "# dis_sim = np.sum(np.abs(features[idx] - features[idy]) / features_stacked.std())\n",
        "print(image_path[idx], image_path[idy], dis_sim, labels[idx] == labels[idy])\n",
        "\n",
        "dis_sim = 1 - cosine_measure(features[1], features[2])\n",
        "# features_stacked = np.stack((features[1], features[2]))\n",
        "# dis_sim = np.sum(np.abs(features[1] - features[2]) / features_stacked.std())\n",
        "print(image_path[1], image_path[2], dis_sim, labels[1] == labels[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6KNQIuFaUWX",
        "outputId": "e9054ee0-3769-4d38-d85c-88c348944bc6"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Market1501/bounding_box_test/1164_c2s2_164077_01.jpg Market1501/bounding_box_test/1016_c1s4_070011_05.jpg 0.8628466882254453 False\n",
            "Market1501/bounding_box_test/0001_c1s1_002301_02.jpg Market1501/bounding_box_test/0001_c1s1_002401_02.jpg 0.3664653089978944 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me calculate the distance between different feature vectors\n",
        "idx = random.randint(0, len(image_path))\n",
        "idy = random.randint(0, len(image_path))\n",
        "dis1 = np.sqrt(np.sum((features[idx] - features[idy]) ** 2))\n",
        "dis2 = np.sqrt(np.sum((features[0] - features[1]) ** 2))\n",
        "print(\"Different labels\", dis1)\n",
        "print(\"Same labels\", dis2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdkJWqfmg5eN",
        "outputId": "839c6867-4d59-4636-8cb5-5c35103715ff"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different labels 286.05017\n",
            "Same labels 208.26337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_path[idx], image_path[idy])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsH0jITotcoe",
        "outputId": "87e9646a-2b8e-45ee-cc7c-7bdb8f529a6f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Market1501/bounding_box_test/1059_c3s2_148819_06.jpg Market1501/bounding_box_test/0924_c3s2_115144_02.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take humongous time to run\n",
        "scores, max_dis_same, min_dis_diff = check_dissimilarity(features, labels)"
      ],
      "metadata": {
        "id": "qTefWuCFNrhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max_dis_same is\", max_dis_same)\n",
        "print(\"min_dis_diff is\", min_dis_diff)"
      ],
      "metadata": {
        "id": "9UGjGoQh5VuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the loss function should consider the pose\n",
        "# Intuitively, we can set the loss function = 0.5 * contrastive loss + 0.5 * pose difference\n",
        "# check the pose difference in manhattan distance\n",
        "\n",
        "\n",
        "pose_image_path = sorted(glob.glob(\"Market1501/bounding_box_test_pose/*.jpg\"))\n",
        "pose_image_path = list(filter(lambda x: x.split(\"/\")[-1][0] != \"-\" and x.split(\"/\")[-1][:4] != \"0000\", pose_image_path))\n",
        "pose1 = transforms.ToTensor()(Image.open(pose_image_path[0]).convert(\"L\")).squeeze().numpy()\n",
        "pose2 = transforms.ToTensor()(Image.open(pose_image_path[1]).convert(\"L\")).squeeze().numpy()\n",
        "pose3 = transforms.ToTensor()(Image.open(pose_image_path[2]).convert(\"L\")).squeeze().numpy()\n",
        "print(\"Difference between {} and {} image pose is {}\".format(pose_image_path[0].split(\"/\")[-1], pose_image_path[1].split(\"/\")[-1], abs(pose1.sum() - pose2.sum())))\n",
        "print(\"Difference between {} and {} image pose is {}\".format(pose_image_path[1].split(\"/\")[-1], pose_image_path[2].split(\"/\")[-1], abs(pose2.sum() - pose3.sum())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbVcYksA-YFp",
        "outputId": "091fd6a3-20a2-49fa-9e00-c3c545760a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between 0001_c1s1_001051_03_rendered.jpg and 0001_c1s1_002301_02_rendered.jpg image pose is 41.8156852722168\n",
            "Difference between 0001_c1s1_002301_02_rendered.jpg and 0001_c1s1_002401_02_rendered.jpg image pose is 5.450984954833984\n"
          ]
        }
      ]
    }
  ]
}